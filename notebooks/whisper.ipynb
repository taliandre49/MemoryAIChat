{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydub import AudioSegment\n",
    "import markdown\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "import os\n",
    "from os import environ\n",
    "import azure\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "import azure\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path)\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/njordan/Downloads/MemoryAIAssistant/testthirty.wav\n",
      "<azure.cognitiveservices.speech.audio.AudioConfig object at 0x119f64210>\n",
      "<azure.cognitiveservices.speech.SpeechConfig object at 0x119f64190>\n",
      "Recognizing speech...\n",
      "Speech recognition canceled: CancellationReason.Error\n",
      "Error details: WebSocket upgrade failed: Authentication error (401). Please check subscription information and region name. SessionId: 8281f90f837a43639425780724df57fb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the Azure Speech Service\n",
    "speech_config = speechsdk.SpeechConfig(subscription= azure_openai_api_key, region=\"eastus\")\n",
    "\n",
    "\n",
    "\n",
    "# Set up the audio input (from microphone or a file)\n",
    "# input_file = \"../AudioFiles/Note_20240229_2305.mp3\"\n",
    "input_file = \"../testthirty.wav\"\n",
    "audio_input = speechsdk.audio.AudioConfig(filename= input_file)\n",
    "\n",
    "print(os.path.abspath(input_file))  # Print the absolute path to verify the file exists\n",
    "print(audio_input)\n",
    "print(speech_config)\n",
    "\n",
    "# Speech recognizer\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n",
    "\n",
    "# Perform speech recognition\n",
    "print(\"Recognizing speech...\")\n",
    "result = speech_recognizer.recognize_once()\n",
    "# result = speech_recognizer.recognize_once_async()\n",
    "\n",
    "# print(result.)\n",
    "if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "    print(\"Recognized: {}\".format(result.text))\n",
    "       # # Save the transcript to a file\n",
    "    output_file_path = f\"testOutPut.txt\"\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(result.text)\n",
    "\n",
    "    print(f\"Transcription saved to {output_file_path}\")\n",
    "\n",
    "elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "    print(\"No speech recognized.\")\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    print(\"Speech recognition canceled: {}\".format(result.cancellation_details.reason))\n",
    "    if result.cancellation_details.error_details:\n",
    "        print(f\"Error details: {result.cancellation_details.error_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     35\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError details: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(cancellation_details.error_details))\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# </SpeechRecognitionWithFile>\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mspeech_recognize_once_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mspeech_recognize_once_from_file\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m audio_input = speechsdk.audio.AudioConfig(filename= input_file)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Creates a speech recognizer using a file as audio input, also specify the speech language\u001b[39;00m\n\u001b[32m     15\u001b[39m speech_recognizer = speechsdk.SpeechRecognizer(\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     speech_config=speech_config, language=\u001b[33m\"\u001b[39m\u001b[33men-US\u001b[39m\u001b[33m\"\u001b[39m, audio_config=\u001b[43maudio_config\u001b[49m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Starts speech recognition, and returns after a single utterance is recognized. The end of a\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# single utterance is determined by listening for silence at the end or until a maximum of about 30\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# seconds of audio is processed. It returns the recognition text as result.\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Note: Since recognize_once() returns only a single utterance, it is suitable only for single\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# shot recognition like command or query.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# For long-running multi-utterance recognition, use start_continuous_recognition() instead.\u001b[39;00m\n\u001b[32m     24\u001b[39m result = speech_recognizer.recognize_once()\n",
      "\u001b[31mNameError\u001b[39m: name 'audio_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up the Azure Speech Service\n",
    "\n",
    "\n",
    "# Set up the audio input (from microphone or a file)\n",
    "# input_file = \"../AudioFiles/Note_20240229_2305.mp3\"\n",
    "\n",
    "\n",
    "def speech_recognize_once_from_file():\n",
    "    \"\"\"performs one-shot speech recognition with input from an audio file\"\"\"\n",
    "    # <SpeechRecognitionWithFile>\n",
    "    speech_config = speechsdk.SpeechConfig(subscription= os.environ[\"AZURE_OPENAI_API_KEY\"], region=\"eastus\")\n",
    "    input_file = \"../segment_1.wav\"\n",
    "    audio_input = speechsdk.audio.AudioConfig(filename= input_file)\n",
    "    # Creates a speech recognizer using a file as audio input, also specify the speech language\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, language=\"en-US\", audio_config=audio_config)\n",
    "\n",
    "    # Starts speech recognition, and returns after a single utterance is recognized. The end of a\n",
    "    # single utterance is determined by listening for silence at the end or until a maximum of about 30\n",
    "    # seconds of audio is processed. It returns the recognition text as result.\n",
    "    # Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "    # shot recognition like command or query.\n",
    "    # For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "    result = speech_recognizer.recognize_once()\n",
    "\n",
    "    # Check the result\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(result.text))\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(result.no_match_details))\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "    # </SpeechRecognitionWithFile>\n",
    "\n",
    "\n",
    "speech_recognize_once_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mp3(input_file, output_folder, segment_length=10):\n",
    "    # Load the MP3 file\n",
    "    audio = AudioSegment.from_mp3(input_file)\n",
    "\n",
    "    # Get the total duration of the audio in milliseconds\n",
    "    total_duration = len(audio)\n",
    "\n",
    "    # Calculate the segment length in milliseconds\n",
    "    segment_length_ms = segment_length * 60 * 1000\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Split the audio into segments\n",
    "    for i, start in enumerate(range(0, total_duration, segment_length_ms)):\n",
    "        end = start + segment_length_ms\n",
    "        segment = audio[start:end]\n",
    "\n",
    "        # Generate the output file name\n",
    "        output_file = os.path.join(output_folder, f\"segment_{i+1}.mp3\")\n",
    "\n",
    "        # Export the segment as a new MP3 file\n",
    "        segment.export(output_file, format=\"mp3\")\n",
    "\n",
    "        print(f\"Segment {i+1} saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1 saved: ../data/segments/segment_1.mp3\n",
      "Segment 2 saved: ../data/segments/segment_2.mp3\n",
      "Segment 3 saved: ../data/segments/segment_3.mp3\n",
      "Segment 4 saved: ../data/segments/segment_4.mp3\n"
     ]
    }
   ],
   "source": [
    "input_file = \"../AudioFiles/Note_20240229_2305.mp3\"\n",
    "output_folder = \"../data/segments\"\n",
    "\n",
    "split_mp3(input_file, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getenv[\"AZURE_OPENAI_API_KEY\"])\n",
    "client = OpenAI(\n",
    "    api_key= os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "# client = openai.OpenAI(\n",
    "#     api_key=environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "#     base_url=environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# client = OpenAI(api_key= os.getenv(\"AZURE_OPENAI_API_KEY\"))\n",
    "# client = AzureOpenAI(\n",
    "#     api_key=environ['AZURE_OPENAI_API_KEY'],\n",
    "#     api_version=\"2024-02-01\",\n",
    "#     azure_endpoint=environ['AZURE_OPENAI_ENDPOINT']\n",
    "# )\n",
    "\n",
    "# print(client)\n",
    "# # List models\n",
    "# try:\n",
    "#     models = client.models.list()\n",
    "#     for model in models.data:\n",
    "#         print(f\"Model ID: {model.id}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error listing models: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file_path, output_file_path):\n",
    "    try:\n",
    "        # Open the audio file\n",
    "        with open(audio_file_path, \"rb\") as audio_file:\n",
    "            # Call the Whisper API\n",
    "            transcription = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file\n",
    "            )\n",
    "\n",
    "        # Extract the transcribed text\n",
    "        transcribed_text = transcription.text\n",
    "\n",
    "        # Save the transcript to a file\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(transcribed_text)\n",
    "\n",
    "        print(f\"Transcription saved to {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/segments/segment_1.mp3\n",
      "../data/transcribes/segment_1.txt\n",
      "An error occurred: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "../data/segments/segment_2.mp3\n",
      "../data/transcribes/segment_2.txt\n",
      "An error occurred: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "../data/segments/segment_3.mp3\n",
      "../data/transcribes/segment_3.txt\n",
      "An error occurred: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n",
      "../data/segments/segment_4.mp3\n",
      "../data/transcribes/segment_4.txt\n",
      "An error occurred: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\n"
     ]
    }
   ],
   "source": [
    "transcribe_path = \"../data/transcribes\"\n",
    "segments_path = \"../data/segments\"\n",
    "\n",
    "os.makedirs(transcribe_path, exist_ok=True)\n",
    "for file in os.listdir(segments_path):\n",
    "    if file.endswith(\".mp3\"):\n",
    "        audio_file_path = os.path.join(segments_path, file)\n",
    "        output_file_path = os.path.join(transcribe_path, file.replace(\".mp3\", \".txt\"))\n",
    "    print(audio_file_path)\n",
    "    print(output_file_path)\n",
    "    transcribe_audio(audio_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the content of all files\n",
    "all_content = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "# get the number of segments\n",
    "segments_count = len([name for name in os.listdir(transcribe_path) if name.endswith(\".txt\")])\n",
    "for i in range(1, segments_count + 1):  # Assuming files are numbered 1 through 8\n",
    "    filename = f\"segment_{i}.txt\"\n",
    "    file_path = os.path.join(transcribe_path, filename)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            all_content.append(content)\n",
    "    else:\n",
    "        print(f\"File {filename} not found.\")\n",
    "\n",
    "# Join all content into a single string\n",
    "combined_content = \"\\n\".join(all_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Generate lecture notes\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m lecture_notes \u001b[38;5;241m=\u001b[39m generate_lecture_notes(\u001b[43mcombined_content\u001b[49m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lecture_notes:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLecture Notes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_content' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to generate lecture notes using GPT-4\n",
    "def generate_lecture_notes(content):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates very well structured lecture notes from transcripts.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Please generate comprehensive lecture notes from the following transcript:\n",
    "\n",
    "                {content}\n",
    "\n",
    "                In creating these notes, please incorporate the following advanced teaching and learning techniques:\n",
    "\n",
    "                1. Organize the content using a clear hierarchical structure (main topics, subtopics, key points).\n",
    "                2. Include a brief summary or learning objectives at the beginning.\n",
    "                3. Use bullet points and numbered lists for easy readability and retention.\n",
    "                4. Incorporate visual elements where appropriate (e.g., diagrams, charts, or mind maps).\n",
    "                5. Highlight key terms, definitions, and important concepts.\n",
    "                6. Add relevant examples and real-world applications to illustrate complex ideas.\n",
    "                7. Include thought-provoking questions or discussion points to encourage critical thinking.\n",
    "                8. Provide analogies or metaphors to explain difficult concepts.\n",
    "                9. Insert brief \"check your understanding\" sections with sample questions or problems.\n",
    "                10. Include mnemonics or memory aids where applicable.\n",
    "                11. Add cross-references to related topics or previous lectures if relevant.\n",
    "                12. Conclude with a summary of main takeaways and potential areas for further exploration.\n",
    "\n",
    "                Feel free to expand on the given content by adding relevant background information, filling in any gaps to ensure completeness, and enriching the material with additional examples or explanations. The goal is to create comprehensive, engaging, and effective lecture notes that facilitate deep understanding and retention of the subject matter. Add at least 5 references for relative materials for readings\"\"\"}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate lecture notes\n",
    "lecture_notes = generate_lecture_notes(combined_content)\n",
    "\n",
    "if lecture_notes:\n",
    "    print(\"Lecture Notes:\")\n",
    "    print(lecture_notes)\n",
    "\n",
    "    # also generate html\n",
    "    html = markdown.markdown(lecture_notes)\n",
    "    \n",
    "    # ave the lecture notes to a file\n",
    "    with open(\"/workspace/data/lecture_notes.txt\", \"w\") as file:\n",
    "        file.write(lecture_notes)\n",
    "    with open(\"/workspace/data/lecture_notes.html\", \"w\") as file:\n",
    "        file.write(html)\n",
    "    print(\"Lecture notes saved to /workspace/data/lecture_notes.txt\")\n",
    "else:\n",
    "    print(\"Failed to generate lecture notes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate lecture notes using GPT-4\n",
    "def generate_lecture_notes_simple(content):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates very well structured lecture notes from transcripts.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Please generate detailed lecture notes from the following transcript:{content}\"}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate lecture notes\n",
    "lecture_notes_simple = generate_lecture_notes_simple(combined_content)\n",
    "\n",
    "if lecture_notes_simple:\n",
    "    # also generate html\n",
    "    html_simple = markdown.markdown(lecture_notes_simple)\n",
    "    \n",
    "    # ave the lecture notes to a file\n",
    "    with open(\"/workspace/data/lecture_notes_simple.txt\", \"w\") as file:\n",
    "        file.write(lecture_notes_simple)\n",
    "    with open(\"/workspace/data/lecture_notes_simple.html\", \"w\") as file:\n",
    "        file.write(html_simple)\n",
    "    print(\"Lecture notes saved to /workspace/data/lecture_notes.txt\")\n",
    "else:\n",
    "    print(\"Failed to generate lecture notes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
